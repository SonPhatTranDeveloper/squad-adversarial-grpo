{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dafddba",
   "metadata": {},
   "source": [
    "# GRPO Training on SQuAD Adversarial Prompter\n",
    "\n",
    "This notebook follows Hugging Face's GRPO guide for TRL and adapts it to our dataset and reward functions. See reference: [Post training an LLM for reasoning with GRPO in TRL](https://huggingface.co/learn/cookbook/en/fine_tuning_llm_grpo_trl).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174f3ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonphat.tran/grpo_adv_prompter_squad/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:grpo_training:Output dir: /Users/sonphat.tran/grpo_adv_prompter_squad/outputs/qwen2-0.5b-grpo\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Export path\n",
    "import sys\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "sys.path.append(\"/Users/sonphat.tran/grpo_adv_prompter_squad\")\n",
    "\n",
    "from src.utils.datasets import load_csv_dataset\n",
    "from src.utils.rewards import bert_reward, format_reward\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"grpo_training\")\n",
    "\n",
    "# Paths and config\n",
    "DATASET_CSV = \"/Users/sonphat.tran/grpo_adv_prompter_squad/data/squad_golden.csv\"\n",
    "OUTPUT_DIR = \"/Users/sonphat.tran/grpo_adv_prompter_squad/outputs/qwen2-0.5b-grpo\"\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "logger.info(\"Output dir: %s\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e9fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:grpo_training:Train rows: 2000\n",
      "INFO:grpo_training:Model with LoRA ready\n"
     ]
    }
   ],
   "source": [
    "# 1) Load dataset and adapt to conversation format expected by TRL GRPO\n",
    "raw_dataset: Dataset = load_csv_dataset(DATASET_CSV)\n",
    "raw_dataset = raw_dataset.shuffle(seed=42)\n",
    "train_dataset = raw_dataset.select(range(min(2000, len(raw_dataset))))\n",
    "logger.info(\"Train rows: %d\", len(train_dataset))\n",
    "\n",
    "# Sanity check columns used by rewards\n",
    "required_cols = [\n",
    "    \"id\",\n",
    "    \"context\",\n",
    "    \"question\",\n",
    "    \"answer\",\n",
    "    \"answer_start_char_idx\",\n",
    "    \"answer_end_char_idx\",\n",
    "    \"prompt\",\n",
    "]\n",
    "missing = [c for c in required_cols if c not in train_dataset.column_names]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns for rewards: {missing}\")\n",
    "\n",
    "# 2) Load base model and tokenizer\n",
    "# We apply LoRA for memory efficiency per HF guide\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Setup LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "logger.info(\"Model with LoRA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e8763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRPOConfig(output_dir='/Users/sonphat.tran/grpo_adv_prompter_squad/outputs/qwen2-0.5b-grpo', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=16, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=1e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/Users/sonphat.tran/grpo_adv_prompter_squad/outputs/qwen2-0.5b-grpo/runs/Sep19_16-52-24_ITVN004103-MAC', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=50, save_total_limit=None, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), parallelism_config=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, hub_revision=None, gradient_checkpointing=True, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, liger_kernel_config=None, eval_use_gather_object=False, average_tokens_across_devices=False, model_init_kwargs=None, disable_dropout=False, max_prompt_length=256, num_generations=4, max_completion_length=96, ds3_gather_for_generation=True, shuffle_dataset=True, generation_batch_size=16, steps_per_generation=16, temperature=1.0, top_p=1.0, top_k=None, min_p=None, generation_kwargs=None, repetition_penalty=1.0, use_transformers_paged=False, cache_implementation=None, use_vllm=False, vllm_mode='server', vllm_model_impl='vllm', vllm_enable_sleep_mode=False, vllm_guided_decoding_regex=None, vllm_server_base_url=None, vllm_server_host='0.0.0.0', vllm_server_port=8000, vllm_server_timeout=240.0, vllm_gpu_memory_utilization=0.3, vllm_tensor_parallel_size=1, beta=0.0, num_iterations=1, epsilon=0.2, delta=None, epsilon_high=None, importance_sampling_level='token', reward_weights=None, scale_rewards='group', loss_type='dapo', mask_truncated_completions=False, sync_ref_model=False, ref_model_mixup_alpha=0.6, ref_model_sync_steps=512, top_entropy_quantile=1.0, use_liger_loss=False, vllm_importance_sampling_correction=True, vllm_importance_sampling_cap=2.0, log_completions=False, num_completions_to_print=None, wandb_log_unique_prompts=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Define GRPO config (adapted from HF guide)\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    remove_unused_columns=False,  # keep extra columns for reward kwargs\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    # Preprocessing controls\n",
    "    max_completion_length=256,  # shorter for quick runs\n",
    "    num_generations=4,\n",
    "    max_prompt_length=512,\n",
    "    # Logging and saving\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ce33ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.trainer.grpo_trainer.GRPOTrainer at 0x173ae1cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Instantiate GRPOTrainer with both rewards\n",
    "# format_reward expects the assistant completion content to match <think>...</think><answer>...</answer>\n",
    "# bert_reward uses the dataset columns to compute QA-based penalties\n",
    "\n",
    "reward_funcs = [format_reward, bert_reward]\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_funcs,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92aad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train and save\n",
    "train_result = trainer.train()\n",
    "logger.info(\"Training complete: %s\", str(train_result))\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "logger.info(\"Saved to %s\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
